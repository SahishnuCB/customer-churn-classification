{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de748322",
   "metadata": {},
   "source": [
    "\n",
    "# Customer Churn Prediction (Binary Classification)\n",
    "\n",
    "This notebook solves a customer churn prediction task using the provided **Customer-Churn.csv** dataset.\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Load and explore the dataset (EDA)\n",
    "2. Clean and preprocess the data\n",
    "3. Build baseline and ML models (Logistic Regression, Random Forest)\n",
    "4. Perform minimal hyperparameter tuning\n",
    "5. Evaluate performance (accuracy, precision, recall, F1, confusion matrix)\n",
    "6. Interpret important features and discuss limitations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d8a58d",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddcefa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547a024b",
   "metadata": {},
   "source": [
    "## 2. Load & Inspect the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67faee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Adjust the path if needed (e.g. when running on Kaggle/Colab)\n",
    "data_path = \"Customer-Churn.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(\"Shape of dataset:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb94761",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Data types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e555c00c",
   "metadata": {},
   "source": [
    "### 2.1 Churn Distribution & Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f19c2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "churn_counts = df['Churn'].value_counts()\n",
    "churn_ratio = df['Churn'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Churn counts:\\n\", churn_counts)\n",
    "print(\"\\nChurn percentage (%):\\n\", churn_ratio.round(2))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(churn_counts.index, churn_counts.values)\n",
    "ax.set_title(\"Churn Distribution\")\n",
    "ax.set_xlabel(\"Churn\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "for i, v in enumerate(churn_counts.values):\n",
    "    ax.text(i, v + 50, str(v), ha='center')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce420ee1",
   "metadata": {},
   "source": [
    "### 2.2 Quick Look at Feature Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e03b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Columns in the dataset:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "demographic_cols = ['gender', 'SeniorCitizen', 'Partner', 'Dependents']\n",
    "service_cols = ['PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity',\n",
    "                'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV',\n",
    "                'StreamingMovies']\n",
    "contract_cols = ['Contract', 'PaperlessBilling', 'PaymentMethod']\n",
    "billing_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "\n",
    "print(\"\\nDemographics:\", demographic_cols)\n",
    "print(\"Services:\", service_cols)\n",
    "print(\"Contract & Payment:\", contract_cols)\n",
    "print(\"Billing:\", billing_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f4da93",
   "metadata": {},
   "source": [
    "### 2.3 Visual Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dca16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tenure vs Churn\n",
    "fig, ax = plt.subplots()\n",
    "df.boxplot(column='tenure', by='Churn', ax=ax)\n",
    "ax.set_title(\"Tenure vs Churn\")\n",
    "ax.set_ylabel(\"Tenure (months)\")\n",
    "plt.suptitle(\"\")\n",
    "plt.show()\n",
    "\n",
    "# MonthlyCharges vs Churn\n",
    "fig, ax = plt.subplots()\n",
    "df.boxplot(column='MonthlyCharges', by='Churn', ax=ax)\n",
    "ax.set_title(\"Monthly Charges vs Churn\")\n",
    "ax.set_ylabel(\"Monthly Charges\")\n",
    "plt.suptitle(\"\")\n",
    "plt.show()\n",
    "\n",
    "# Contract type vs Churn\n",
    "contract_churn = pd.crosstab(df['Contract'], df['Churn'])\n",
    "contract_churn_norm = contract_churn.div(contract_churn.sum(axis=1), axis=0)\n",
    "\n",
    "print(\"Contract vs Churn counts:\")\n",
    "print(contract_churn)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "bottom = np.zeros(len(contract_churn_norm.index))\n",
    "for col in contract_churn_norm.columns:\n",
    "    ax.bar(contract_churn_norm.index,\n",
    "           contract_churn_norm[col].values,\n",
    "           bottom=bottom,\n",
    "           label=col)\n",
    "    bottom += contract_churn_norm[col].values\n",
    "ax.set_title(\"Contract Type vs Churn (Proportions)\")\n",
    "ax.set_ylabel(\"Proportion\")\n",
    "ax.legend(title=\"Churn\")\n",
    "plt.xticks(rotation=15)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9b7c1d",
   "metadata": {},
   "source": [
    "### 2.4 Correlation of Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629ad3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numeric_cols = ['SeniorCitizen', 'tenure', 'MonthlyCharges']\n",
    "\n",
    "# TotalCharges is still object for now, so we don't include it yet\n",
    "corr = df[numeric_cols].corr()\n",
    "print(corr)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "cax = ax.matshow(corr, vmin=-1, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "ticks = np.arange(0, len(numeric_cols), 1)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xticklabels(numeric_cols, rotation=45, ha='left')\n",
    "ax.set_yticklabels(numeric_cols)\n",
    "ax.set_title(\"Correlation Heatmap (Numeric Features)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c498d0",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1238266d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_clean = df.copy()\n",
    "\n",
    "# 3.1 Clean TotalCharges (string-based numeric with possible spaces)\n",
    "df_clean['TotalCharges'] = pd.to_numeric(df_clean['TotalCharges'], errors='coerce')\n",
    "print(\"Number of missing TotalCharges after conversion:\", df_clean['TotalCharges'].isna().sum())\n",
    "\n",
    "# Drop rows where TotalCharges could not be converted (very small fraction)\n",
    "df_clean = df_clean.dropna(subset=['TotalCharges'])\n",
    "\n",
    "# 3.2 Simplify 'No phone service' / 'No internet service' categories\n",
    "service_replace_cols = ['MultipleLines', 'OnlineSecurity', 'OnlineBackup',\n",
    "                        'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies']\n",
    "\n",
    "for col in service_replace_cols:\n",
    "    df_clean[col] = df_clean[col].replace({'No internet service': 'No',\n",
    "                                           'No phone service': 'No'})\n",
    "\n",
    "# 3.3 Encode target\n",
    "df_clean['Churn'] = df_clean['Churn'].map({'No': 0, 'Yes': 1})\n",
    "y = df_clean['Churn']\n",
    "\n",
    "# 3.4 Drop identifier\n",
    "df_clean = df_clean.drop(columns=['customerID'])\n",
    "\n",
    "# 3.5 Encode binary categorical variables\n",
    "binary_cols = ['gender', 'Partner', 'Dependents', 'PhoneService', 'PaperlessBilling'] + service_replace_cols\n",
    "\n",
    "# Map yes/no\n",
    "yes_no_map = {'Yes': 1, 'No': 0}\n",
    "for col in binary_cols:\n",
    "    if col == 'gender':\n",
    "        df_clean[col] = df_clean[col].map({'Female': 0, 'Male': 1})\n",
    "    else:\n",
    "        df_clean[col] = df_clean[col].map(yes_no_map)\n",
    "\n",
    "# 3.6 One-hot encode multi-category features\n",
    "multi_cat_cols = ['InternetService', 'Contract', 'PaymentMethod']\n",
    "\n",
    "X = df_clean.drop(columns=['Churn'])\n",
    "X = pd.get_dummies(X, columns=multi_cat_cols, drop_first=True)\n",
    "\n",
    "print(\"Shape after preprocessing:\", X.shape)\n",
    "print(\"Remaining dtypes:\")\n",
    "print(X.dtypes.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3a745b",
   "metadata": {},
   "source": [
    "### 3.1 Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af05c9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
    "print(\"Churn proportion in train:\")\n",
    "print(y_train.value_counts(normalize=True).round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27100fad",
   "metadata": {},
   "source": [
    "## 4. Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e162712a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "baseline_clf = DummyClassifier(strategy='most_frequent', random_state=RANDOM_STATE)\n",
    "baseline_clf.fit(X_train, y_train)\n",
    "y_pred_baseline = baseline_clf.predict(X_test)\n",
    "\n",
    "print(\"Baseline (Most Frequent) Metrics:\")\n",
    "print(\"Accuracy :\", accuracy_score(y_test, y_pred_baseline).round(4))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_baseline, zero_division=0).round(4))\n",
    "print(\"Recall   :\", recall_score(y_test, y_pred_baseline, zero_division=0).round(4))\n",
    "print(\"F1       :\", f1_score(y_test, y_pred_baseline, zero_division=0).round(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf15231",
   "metadata": {},
   "source": [
    "## 5. Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8947977e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_reg = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced',\n",
    "    solver='liblinear',\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred_lr = log_reg.predict(X_test)\n",
    "\n",
    "print(\"Logistic Regression Metrics:\")\n",
    "print(\"Accuracy :\", accuracy_score(y_test, y_pred_lr).round(4))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_lr).round(4))\n",
    "print(\"Recall   :\", recall_score(y_test, y_pred_lr).round(4))\n",
    "print(\"F1       :\", f1_score(y_test, y_pred_lr).round(4))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e2c711",
   "metadata": {},
   "source": [
    "## 6. Random Forest with Minimal Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafefaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf = RandomForestClassifier(\n",
    "    random_state=RANDOM_STATE,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best CV F1 score:\", grid_search.best_score_.round(4))\n",
    "\n",
    "rf_best = grid_search.best_estimator_\n",
    "\n",
    "y_pred_rf = rf_best.predict(X_test)\n",
    "\n",
    "print(\"\\nRandom Forest (Tuned) Metrics:\")\n",
    "print(\"Accuracy :\", accuracy_score(y_test, y_pred_rf).round(4))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_rf).round(4))\n",
    "print(\"Recall   :\", recall_score(y_test, y_pred_rf).round(4))\n",
    "print(\"F1       :\", f1_score(y_test, y_pred_rf).round(4))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b558c3d",
   "metadata": {},
   "source": [
    "## 7. Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a489a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = {\n",
    "    \"Baseline (Most Frequent)\": y_pred_baseline,\n",
    "    \"Logistic Regression\": y_pred_lr,\n",
    "    \"Random Forest (Tuned)\": y_pred_rf\n",
    "}\n",
    "\n",
    "for name, y_pred in models.items():\n",
    "    print(f\"\\n{name}\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"No churn\", \"Churn\"])\n",
    "    disp.plot(values_format='d')\n",
    "    plt.title(f\"Confusion Matrix - {name}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dee4e5",
   "metadata": {},
   "source": [
    "## 8. Feature Importance (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9fa75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "importances = rf_best.feature_importances_\n",
    "feature_importance = pd.Series(importances, index=X_train.columns).sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 15 features by importance:\")\n",
    "print(feature_importance.head(15))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "top_n = 15\n",
    "ax.barh(feature_importance.head(top_n).index[::-1],\n",
    "        feature_importance.head(top_n).values[::-1])\n",
    "ax.set_title(\"Top Feature Importances (Random Forest)\")\n",
    "ax.set_xlabel(\"Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7be7b2",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Interpretation & Discussion\n",
    "\n",
    "**Which features influence churn?**  \n",
    "\n",
    "From the feature importance and coefficients (Random Forest & Logistic Regression), we can usually observe that:\n",
    "\n",
    "* **Contract type**: Month-to-month contracts tend to have **higher churn**, while longer-term contracts (1 or 2 year) are associated with more stable customers.\n",
    "* **Tenure**: Customers with a **shorter tenure** (newer customers) are more likely to churn; long-term customers tend to stay.\n",
    "* **Monthly and total charges**: Higher **MonthlyCharges** often correlate with churn, especially when customers perceive the service as too expensive for the value received.\n",
    "* **Internet-related services** (e.g., Fiber optic, lack of OnlineSecurity/TechSupport) often play a role, as customers who use many services but are unhappy with quality/price may churn.\n",
    "\n",
    "---\n",
    "\n",
    "### Limitations & Class Imbalance Effects\n",
    "\n",
    "* The dataset is moderately imbalanced (churners are the minority).  \n",
    "  * We mitigated this via **class_weight='balanced'** for Logistic Regression and Random Forest.\n",
    "* Metrics like **recall and F1** are more informative than raw accuracy in this setting.\n",
    "* We performed only **minimal hyperparameter tuning**; more extensive tuning (e.g., RandomizedSearchCV, more parameters) could improve performance.\n",
    "* We used simple one-hot encoding and standard models. More advanced approaches (e.g. gradient boosting like XGBoost/LightGBM, or neural networks) might yield better performance.\n",
    "* There may still be some mild data leakage from encoding categories using the full dataset, although the risk is small. In production, using pipelines that fit encoders only on the training set is recommended.\n",
    "\n",
    "---\n",
    "\n",
    "### Possible Improvements\n",
    "\n",
    "1. Use cross-validation and more robust hyperparameter tuning for all models.\n",
    "2. Try more advanced models (Gradient Boosted Trees, XGBoost, CatBoost).\n",
    "3. Explore feature engineering (e.g., bucketizing tenure, interaction terms).\n",
    "4. Apply resampling techniques (SMOTE, undersampling) and compare with class weights.\n",
    "5. Deploy as an API and monitor performance on real-time data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
